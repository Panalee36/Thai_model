import pandas as pd
import os
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import sys

# Setup import path for utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.text_processing import preprocess_text

# === 1. Setup Path (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå) ===
# ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡∏≠‡∏¢‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏≤ backend
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_DIR = os.path.dirname(CURRENT_DIR)
DATA_DIR = os.path.join(BACKEND_DIR, "data")
MODELS_DIR = os.path.join(BACKEND_DIR, "models")
OUTPUT_MODEL_DIR = os.path.join(MODELS_DIR, "my_thai_news_model")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
if not os.path.exists(MODELS_DIR):
    os.makedirs(MODELS_DIR)

# === 2. Config Device (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠) ===
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏µ GPU ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡πâ‡∏≤‡∏á (Mac M1/M2 ‡πÉ‡∏ä‡πâ mps)
if torch.cuda.is_available():
    device = "cuda"
    print("üöÄ Using GPU (CUDA)")
elif torch.backends.mps.is_available():
    device = "mps"
    print("üçé Using Mac GPU (MPS) - ‡πÅ‡∏£‡∏á‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô!")
else:
    device = "cpu"
    print("üê¢ Using CPU (Might be slow)")

# === 3. Load & Prepare Data (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ===
print(f"üìÇ Loading Data from: {DATA_DIR}")
csv_path = os.path.join(DATA_DIR, '11.agnews_thai_test_hard.csv')

try:
    df = pd.read_csv(csv_path)
except FileNotFoundError:
    print("‚ùå ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå CSV! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå 11.agnews_thai_test_hard.csv ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô folder 'backend/data' ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á")
    exit()

# ‡∏£‡∏ß‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Preprocessing
df['text'] = (df['headline'] + " " + df['body']).apply(preprocess_text)
label_map = {'World': 0, 'Business': 1, 'SciTech': 2}
df['label'] = df['topic'].map(label_map)

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train 80% / Validation 20%
train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Format ‡∏Ç‡∏≠‡∏á Hugging Face Dataset
train_dataset = Dataset.from_pandas(pd.DataFrame({'text': train_texts, 'label': train_labels}))
val_dataset = Dataset.from_pandas(pd.DataFrame({'text': val_texts, 'label': val_labels}))

# === 4. Tokenizer (‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥) ===
MODEL_NAME = "airesearch/wangchanberta-base-att-spm-uncased"
print(f"‚¨áÔ∏è Downloading Tokenizer: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_function(examples):
    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Padding ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà 128)
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

print("‚öôÔ∏è Tokenizing data...")
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_val = val_dataset.map(tokenize_function, batched=True)

# === 5. Model Setup (‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•) ===
print("‚¨áÔ∏è Downloading Model...")
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)
model.to(device) # ‡∏™‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏ó‡∏µ‡πà GPU/MPS

# === 6. Training Arguments (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô) ===
training_args = TrainingArguments(
    output_dir="./results_temp",    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    num_train_epochs=3,             # ‡πÄ‡∏ó‡∏£‡∏ô 3 ‡∏£‡∏≠‡∏ö (‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡πâ‡∏≠‡∏ô‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 1-2 ‡πÑ‡∏î‡πâ)
    per_device_train_batch_size=8,  # ‡∏Ç‡∏ô‡∏≤‡∏î Batch (‡∏ñ‡πâ‡∏≤ RAM ‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 4)
    per_device_eval_batch_size=8,
    
    evaluation_strategy="epoch",          
    
    save_strategy="no",             # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ã‡∏ü Checkpoint ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏≤‡∏á (‡πÄ‡∏õ‡∏•‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà)
    learning_rate=2e-5,             # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
)

# === 7. Start Training (‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô) ===
print("üöÄ Start Training... (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏Å‡∏û‡∏±‡∏Å ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏≤‡πÅ‡∏ü‡∏£‡∏≠‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‚òï)")
trainer.train()

# === 8. Save Model (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå) ===
print(f"üíæ Saving model to: {OUTPUT_MODEL_DIR}")
model.save_pretrained(OUTPUT_MODEL_DIR)
tokenizer.save_pretrained(OUTPUT_MODEL_DIR)

print("üéâ Training Complete! WangchanBERTa is ready for action.")